{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Introduction to Python for Data Science__\n",
    "## _CSE Mentor Program - University of Colorado, Denver. Spring-2019_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This workshop is intended to introduce Python to Undergrad and Graduate students in the context of Data Science techniques. \n",
    "\n",
    "During three sessions we will covering the basis of the Python Language, the use of Pandas to access and manipulate data and the Scikit-Learn library to do some basic analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 3 - Introduction to ScikitLearn\n",
    "In this session we will focus on SciKit Learn, a library specialized in machine learning and data science tools. \n",
    "We will cover only a few models in this workshop, however, it will illustrate the process of doing data analysis but not covering the algorithms formal model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit Learn\n",
    "\n",
    "Now we will focus on using a library to, using probabilitics models, classify or predict new values based on a historical training sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets # contains sample datases. \n",
    "import sklearn.metrics as metrics  # a module to measure how well out model is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Classifying Flowers using Gaussian Naive-Bayes\n",
    "\n",
    "In machine learning, Naive Bayes classifiers are a family of simple \"probabilistic classifiers\" based on applying Bayes' theorem with strong (naive) independence assumptions between the features.\n",
    "\n",
    "We call a feature to every single characteristic of our dataset, like age, gender, heigth, weigth, etc. \n",
    "\n",
    "__The Gaussian Naive-Bayes relays in the following equiation__\n",
    "\\begin{equation*}\n",
    "P(x_i \\mid y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_y}} \\exp\\left(-\\frac{(x_i - \\mu_y)^2}{2\\sigma^2_y}\\right)\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris['DESCR'])\n",
    "print(\"=\"*30)\n",
    "print(\"There are {} iris data points.\".format(len(iris.data)))\n",
    "print(\"Features\",iris.feature_names)\n",
    "print(\"Data\",iris.data[:5])\n",
    "print(\"Target Names:\",iris.target_names)\n",
    "print(\"Target\",iris.target[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "#### Create a model and train it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()  #Gaussian Naive-Bayes\n",
    "\n",
    "## Fit the Model\n",
    "gnb.fit(iris.data, iris.target)\n",
    "\n",
    "#Predict the Lilly \n",
    "type_predictions = gnb.predict(iris.data)\n",
    "\n",
    "type_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(45,80):\n",
    "    real_type      = iris.target_names[iris.target[i]]\n",
    "    predicted_type = iris.target_names[type_predictions[i]]\n",
    "    \n",
    "    text = \"Specimen \"+str(i)+ \" predicted \" + predicted_type + \" real type is \" + real_type +\".\"\n",
    "    \n",
    "    if real_type != predicted_type:\n",
    "        text += \" The Prediction was NOT CORRECT.\"\n",
    "    \n",
    "    print (text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_points  = iris.data.shape[0]  # is the same as len(iris.data)\n",
    "mislabeled_points = (iris.target != type_predictions).sum()\n",
    "print(\"Number of mislabeled points out of a total {} points were: {}\".format(number_of_points,mislabeled_points))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Metrics on the predictions\n",
    "\n",
    "#### Accuracy\n",
    "Accuracy is the proximity of measurement results to the true value; precision, the repeatability, or reproducibility of the measurement.\n",
    "\n",
    "#### Recall\n",
    "The recall is the ratio tp / (tp + fn) where tp is the number of true positives and fn the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples.\n",
    "The best value is 1 and the worst value is 0.\n",
    "\n",
    "#### F1\n",
    "F1 score is a measure of a test accuracy. It considers both the precision p and the recall r of the test to compute the score\n",
    "\n",
    "\\begin{equation*}\n",
    "F1 = 2 \\times \\frac{precision * recall }{precision + recall}\n",
    "\\end{equation*}\n",
    "\n",
    "<img width=30% src=\"./files/recall.png\">\n",
    "<img width=50% src=\"./files/accuracy_and_all.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_values = iris.target\n",
    "print(\"Accuracy is {}\".format(metrics.accuracy_score(true_values, type_predictions)))\n",
    "print(\"Recall is {}\".format  (metrics.recall_score  (true_values, type_predictions, average=\"weighted\", labels=np.unique(true_values) ) ) )\n",
    "print(\"F1-Score is {}\".format(metrics.f1_score      (true_values, type_predictions, average=\"weighted\", labels=np.unique(true_values) ) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the issues in our code?\n",
    "\n",
    "Well, we are training our model with all out data, so the accuracy and recall is expected to be high, as our model is \"perfectly\" fit for our data. This issue is called __overfitting__. \n",
    "\n",
    "We usually, try to split our sample intro a train and test datasets to get a better picture of what is going on. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(iris.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()  #Gaussian Naive-Bayes\n",
    "\n",
    "## Fit the Model\n",
    "gnb.fit(iris.data[:120], iris.target[:120])\n",
    "\n",
    "#Predict the Lilly \n",
    "type_predictions = gnb.predict(iris.data[120:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_values = iris.target[120:]\n",
    "print(\"Accuracy is {}\".format(metrics.accuracy_score(true_values, type_predictions) ))\n",
    "print(\"Recall is {}\".format(  metrics.recall_score  (true_values, type_predictions, average=\"weighted\", labels=np.unique(true_values)) ))\n",
    "print(\"F1-Score is {}\".format(metrics.f1_score      (true_values, type_predictions, average=\"weighted\", labels=np.unique(true_values)) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handwriting Recognition using SVM\n",
    "\n",
    "SVM (support vector machine) is a machine learning supervised classification algorithm. \n",
    "Given a set of training examples, each marked as belonging to a category, an SVM training algorithm builds a model that assigns new examples to a category, making it a non-probabilistic binary linear classifier.\n",
    "\n",
    "In other words, the SVM sepparates the points using a line.\n",
    "\n",
    "<span style=\"border:5px solid black\"><img src=\"./files/Kernel_Machine.png\" width=40% style=\"width:20%;border:5px solid black\"></span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = datasets.load_digits()\n",
    "print(digits['DESCR'])\n",
    "print(\"=\"*30)\n",
    "print(\"There are {} digits data points.\".format(len(digits.data)))\n",
    "print(\"Data\",digits.data[:5])\n",
    "print(\"Target Names:\",digits.target_names)\n",
    "print(\"Target\",digits.target[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageId = -1\n",
    "img2 = (digits.images[imageId]).reshape([8,8])\n",
    "plt.imshow(img2,cmap=plt.cm.gray_r)   #gray_r reversed, so 255 is black instead of white\n",
    "plt.show()\n",
    "print (\"the image represent\",digits.target[imageId])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Creating our SVM model \n",
    "#\n",
    "clf = svm.SVC(gamma=0.001, C=100.)   #C-Support Vector Classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The array has\",len(digits.data),\"images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# TRAINING PHASE\n",
    "#\n",
    "#using the first 1500 images for training the model and the rest for testing.\n",
    "\n",
    "clf.fit(digits.data[:1500], digits.target[:1500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#  PREDICITON PHASE\n",
    "#\n",
    "def predict(imageID,display=False):\n",
    "    if display:\n",
    "        img2 = (digits.images[imageID]).reshape([8,8])\n",
    "        plt.imshow(img2,cmap=plt.cm.gray_r)\n",
    "        plt.show()\n",
    "    prediction = clf.predict(digits.data[imageID:imageID+1])[0]\n",
    "    text = \"it is predicted to be a:\"+str(prediction)+\", when it really is a: \"+str(digits.target[imageID:imageID+1][0])\n",
    "    return prediction == digits.target[imageID:imageID+1][0], text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1501,1796):\n",
    "    ok, text = predict(i)\n",
    "    if not ok:\n",
    "        print(\">>> id: \"+str(i)+\" <<<\",text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(1573,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering for Wine Classification.\n",
    "\n",
    "Clustering is a technique that tries to sepparate the dataset into _n_ subsets. For each subset it define a center that defines the characteristics of the subset. Each point in the dataset, will be assigned to the subset (cluster) that minimizes the distance with the cluster center.\n",
    "\n",
    "There are several techniques to implement clustering. The most used are K-Means (and its variants) and DB-Scan.\n",
    "\n",
    "\n",
    "<span style=\"border:5px solid black\"><img src=\"./files/k-means.jpg\" width=30% style=\"width:50%;border:5px solid black\"></span>\n",
    "<span style=\"border:5px solid black\"><img src=\"./files/dbscan.png\" width=30% style=\"width:50%;border:5px solid black\"></span>\n",
    "<span style=\"border:5px solid black\"><img src=\"./files/dbscan2.png\" width=30% style=\"width:50%;border:5px solid black\"></span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine = datasets.load_wine()\n",
    "print(wine[\"DESCR\"])\n",
    "print(\"=\"*30)\n",
    "print(\"There are {} wine sample data points.\".format(len(wine.data)))\n",
    "print(\"Data\",wine.data[:5])\n",
    "print(\"Target Names:\",wine.target_names)\n",
    "print(\"Target\",wine.target[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Wine Features\")\n",
    "print(\"-------------\")\n",
    "for i,data in enumerate(wine.feature_names):\n",
    "    print(i,data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Let's use the first two features only (alcohol & malic_acid)\n",
    "#\n",
    "wine2d = np.asarray(np.asmatrix(wine.data)[:,:2])\n",
    "wine2d[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=3, random_state=0).fit(wine2d)\n",
    "print(\"Labels\\n\",kmeans.labels_)\n",
    "print(\"Centers\\n\",kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.asarray(np.asmatrix(wine2d)[:,:1]).reshape(len(wine2d))\n",
    "y = np.asarray(np.asmatrix(wine2d)[:,1:2]).reshape(len(wine2d))\n",
    "colorLabel = kmeans.labels_\n",
    "colorMap   = matplotlib.colors.ListedColormap(['green','blue','orange'])\n",
    "\n",
    "\n",
    "\n",
    "centroids_x = np.asarray(np.asmatrix(kmeans.cluster_centers_)[:,:1])\n",
    "centroids_y = np.asarray(np.asmatrix(kmeans.cluster_centers_)[:,1:2])\n",
    "\n",
    "\n",
    "\n",
    "fig = plt.figure(1, figsize=(8,8))     #Figure of 9inches wide, 3 inches tall.\n",
    "\n",
    "# equivalent but more general\n",
    "ax=fig.add_subplot(1, 1, 1)\n",
    "\n",
    "ax.set_xlabel(wine.feature_names[0])\n",
    "ax.set_ylabel (wine.feature_names[1])\n",
    "\n",
    "ax.scatter(x,y, c=colorLabel, cmap=colorMap , s=20 )\n",
    "ax.scatter(centroids_x,centroids_y, s=100, c=\"red\")\n",
    "\n",
    "ax.scatter([13,14,12.75],[5,1,1], s=100, c=\"black\",marker=\"^\")\n",
    "ax.annotate(\"(13,5)\",(13,5))\n",
    "ax.annotate(\"(14,1)\",(14,1))\n",
    "ax.annotate(\"(12.75,1)\",(12.75,1))\n",
    "ax.minorticks_on()\n",
    "\n",
    "plt.ylim(bottom=0)\n",
    "plt.xlim(left=10,right=16)\n",
    "plt.title(\"My Scatter Plot\")\n",
    "\n",
    "custom_lines = [matplotlib.lines.Line2D([0], [0], marker='o', color=\"green\", lw=0),\n",
    "                matplotlib.lines.Line2D([0], [0], marker='o', color=\"blue\", lw=0),\n",
    "               matplotlib.lines.Line2D([0], [0], marker='o', color=\"orange\", lw=0)]\n",
    "\n",
    "ax.legend(custom_lines, ['Cluster_1','Cluster_2','Cluster_3'])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_classes = [[13,5],[14,1],[12.75,1]]\n",
    "for value in predict_classes:\n",
    "    predicted_class = kmeans.predict([value])[0]\n",
    "    centroid = kmeans.cluster_centers_[predicted_class]\n",
    "    print(\"Kmeans predicts for point {:^11} to belong a class {} in the cluster with center: {}\".format(str(value), predicted_class, centroid))         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-'*120)\n",
    "print(\"Using Kmeans with 3 clusters, with 2 features, we achieved an accuracy of {}\".format(metrics.accuracy_score( wine.target,kmeans.labels_)))\n",
    "print('-'*120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_size=0\n",
    "best_acc=0\n",
    "for i in range(1,11):\n",
    "    kmeans = KMeans(n_clusters=i, random_state=0).fit(wine2d)\n",
    "    accuracy = metrics.accuracy_score( wine.target,kmeans.labels_)\n",
    "    if accuracy>best_acc:\n",
    "        best_acc= accuracy\n",
    "        best_size=i\n",
    "    print('-'*120)\n",
    "    print(\"Using Kmeans with {} clusters, with two features we achieved an accuracy of {}\".format(i,accuracy))\n",
    "    \n",
    "print('-'*120,\"\\n\")\n",
    "print(\"*\"*120)\n",
    "print(\"Best results were achieved with {} clusters reaching a {} of accuracy.\".format(best_size, best_acc))\n",
    "print(\"*\"*120,\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's use all the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=3, random_state=0).fit(wine.data)\n",
    "print(\"Labels\\n\",\"-\"*30,'\\n',kmeans.labels_)\n",
    "print(\"Centers\\n\",\"-\"*30,'\\n',kmeans.cluster_centers_)\n",
    "print('-'*120)\n",
    "print(\"Using Kmeans with 3 clusters, with all the features we achieved an accuracy of {}\".format(metrics.accuracy_score( wine.target,kmeans.labels_)))\n",
    "print('-'*120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_size=0\n",
    "best_acc=0\n",
    "for i in range(1,11):\n",
    "    kmeans = KMeans(n_clusters=i, random_state=0).fit(wine.data)\n",
    "    accuracy = metrics.accuracy_score( wine.target,kmeans.labels_)\n",
    "    if accuracy>best_acc:\n",
    "        best_acc= accuracy\n",
    "        best_size=i\n",
    "    print('-'*120)\n",
    "    print(\"Using Kmeans with {} clusters, with all the features we achieved an accuracy of {}\".format(i,accuracy))\n",
    "    \n",
    "print('-'*120,\"\\n\")\n",
    "print(\"*\"*120)\n",
    "print(\"Best results were achieved with {} clusters reaching a {} of accuracy.\".format(best_size, best_acc))\n",
    "print(\"*\"*120,\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine2d = np.asarray(np.asmatrix(wine.data)[:,[10,6,9]])\n",
    "best_size=0\n",
    "best_acc=0\n",
    "for i in range(1,11):\n",
    "    kmeans = KMeans(n_clusters=i, random_state=0).fit(wine2d)\n",
    "    accuracy = metrics.accuracy_score( wine.target,kmeans.labels_)\n",
    "    if accuracy>best_acc:\n",
    "        best_acc= accuracy\n",
    "        best_size=i\n",
    "    print('-'*120)\n",
    "    print(\"Using Kmeans with {} clusters, with two features we achieved an accuracy of {}\".format(i,accuracy))\n",
    "    \n",
    "print('-'*120,\"\\n\")\n",
    "print(\"*\"*120)\n",
    "print(\"Best results were achieved with {} clusters reaching a {} of accuracy.\".format(best_size, best_acc))\n",
    "print(\"*\"*120,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
